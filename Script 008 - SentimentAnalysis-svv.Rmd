---
title: "Analisis de Sentimiento para Decisiones de Inversion"
author: "Ramean Team"
date: "22-11-2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## GitHub Documents

This is an R Markdown format used for publishing markdown documents to GitHub. When you click the **Knit** button all R code chunks are run and a markdown file (.md) suitable for publishing to GitHub is generated.

### Cargar librerias
```{r Carga de librerias}
library(rtweet)
library(dplyr)
library(tidytext)
library(ggplot2)
```


### Twitter API Token 
Utilizaremos datos de API de las cuentas de Hamlet y Sebasti√°n
```{r Generaci√≥n de tokens}
#####################################
######### DATOS API HAMLET ##########
#####################################
app_h = "app4data" #app4data
consumer_key_h = "yT0a2vvDkbmquBoKiWILt2jdx"
consumer_secret_h = "baeaFoj0fv3TTGS98PTTKkg29r4Eh5FwlePgtjQLGIzNSDLUQk"
access_token_h = "803590273-vXRZOdHPb5cMDuusJ3Z6VPDBqB2nRLIi84l7Yh34"
access_secret_h = "yZVU0mwzHsItUulpvEibo8XV4Y0A04pXu9BRmEcaq7AuD"

token_h <- create_token(app = app_h,
                        consumer_key = consumer_key_h,
                        consumer_secret = consumer_secret_h,
                        access_token = access_token_h,
                        access_secret = access_secret_h)


#####################################
######## DATOS API SEBASTIAN ########
#####################################
app_svv = "EstallidoSocial" #EstallidoSocial
consumer_key_svv = "394F9QeZbkwnc7pkUEOBRBTzo"
consumer_secret_svv = "8OEoJnirBhfTeZJ5dE5G2BJsodQQNVlV0MFDxuXa80dKUa65F1"
access_token_svv = "18360380-4fVpZWre5Ycv80P5nTnHL032NlBld0ibUw0WItAhX"
access_secret_svv = "dfv6PKGh3efO5g98Lp7DxTTRuVvTg2RPd6KjB3kwUKrez"

token_s <- create_token(app = app_svv, 
                        consumer_key = consumer_key_svv,
                        consumer_secret = consumer_secret_svv,
                        access_token = access_token_svv,
                        access_secret = access_secret_svv)

token_h
token_s
```


### Palabras clave
Se utilizan las siguientes palabras claves para las b√∫squedas
```{r}
query_chile <- "chile OR estallido OR social OR pacos OR constitucion OR DDHH OR dictadura OR pinochet OR represion OR derechos OR protesta OR marcha OR AFP OR salud OR educaci√≥n OR ojos OR ocular OR disparar OR desperto OR justicia"

query_4w <- "chile + estallido + social + pacos + constitucion + DDHH + dictadura + pinochet + represion + derechos + protesta + marcha + AFP + salud + ojos + ocular + disparar + desperto + justicia"

query_4w_test <- "chile + renuncia"
```


```{r Geopoligono r√°pido de Chile}
#Polygon_0
# -18.4077179152925, -70.74981321183628
# -17.231075173517706, -69.04693235246128
# -19.388647922557066, -68.21471799699253
# -22.53642555949104, -67.51433957902378
# -22.79747232096398, -66.64092649308628
# -24.174260055271485, -67.05143577165938
# -24.820504140447383, -67.98928880023243
# -27.23537039972445, -68.09218089130547
# -29.347220236543446, -68.934642561842
# -34.006493299300715, -69.14525797947613
# -37.313468661245956, -70.23477964711026
# -42.780001518194844, -70.84090287724445
# -46.5692856176756, -70.74845667981151
# -48.72014422474472, -71.66903045609502
# -50.10189182584807, -72.56877046923682
# -50.63676955220834, -71.69507266002228
# -51.49512661069634, -71.34871860080773
# -51.96010704322915, -68.36564579159312
# -54.65162243959103, -67.92879688698582
# -54.886640758789646, -65.9525599346822
# -56.1129162709928, -66.96460423237858
# -55.50792041611683, -71.29321751362858
# -53.32870986714529, -75.03955540425358
# -48.941772237652835, -76.58862766987858
# -37.42153260673602, -74.28445481585743
# -29.568622792385753, -72.42924338884688
# -28.1262235752008, -111.72377347871793
# -25.94123149615449, -111.57976267534161
# -27.21262528345972, -72.44408481858898


# geocl <- lookup_coords("chile", apikey = "AIzaSyAYOPrmh2I18KXT4mU8zI5jNhuTSen9Ve8")
# geocl
# 
# geocl
```

### IMPORTANTE!!! Carga de base de tweets desde archivo (en caso que exista)
Se cargan los diferentes archivos que contienen las bases de tweets recopilados desde twitter
```{r Carga de base de tweets desde archivo (en caso que exista)}

### Descomentar y ejecutar estas lineas si se cuenta con los archivos en la carpeta del proyecto

tuits_s <- readRDS("tuits_s.rds")
tuits_r <- readRDS("tuits_r.rds")
tuits_r_df <- readRDS("tuits_r_df.rds")
```


### Se realiza prueba de extracci√≥n de tweets con la libreria rtweet 
usando palabras claves, solo para comprobar que conectividad y API est√°n funcionando correctamente.
```{r}
# search_tweets(q = query_chile,
#               n = 100,
#               type = "recent",
#               include_rts = TRUE,
#               geocode = NULL,
#               max_id = NULL,
#               parse = TRUE,
#               token = NULL,
#               retryonratelimit = FALSE,
#               verbose = TRUE
#               )
# 
# 
# start_time <- Sys.time(); start_time
# tuitstest <- search_tweets(q = query_chile3, token = token_h, n = 180, lang = "es")
# end_time <- Sys.time()
# diff <- end_time - start_time
# diff
# tuitstest
```

### Se realiza extracci√≥n definitiva de tweets 
Por limitaciones de la librer√???a en versi√≥n estandar, s√≥lo se pueden extraer tweets de hasta hace 9 d√???as.
Algunas condiciones:
- Se extraen tweets populares y recientes ("mixed"), 
- no se incluyen retweets, 
- se acota la b√∫squeda a lenguaje espa√±ol 
- se acota pa√???s a Chile.
- Se extrae el m√°ximo n√∫mero de tweets que permite esta API (18.000)

Se mide el tiempo del proceso, tomando alrededor de 16 minutos.
```{r}
start_time <- Sys.time()

tuits_s <- search_tweets(q = query_chile3, 
                       n = 18000, 
                       type = "mixed", 
                       include_rts = F, 
                       token = token_s, 
                       retryonratelimit = T, 
                       lang = "es",
                       country = "chile"
                       )

end_time <- Sys.time()
saveRDS(tuits_s, file = "tuits_s.rds")
diff <- end_time - start_time
diff
tuits_s
```
### Se revisan dataframes de tweets
```{r}
tuitstest
tuits_s
```

## Ejecutar esto sÛlo si eres Hamlet.
```{r}
setwd("C:\\Hamlet\\Personal\\Data Science\\Diplomado DataScience UC\\2019 Hamlet\\Proyecto Final\\Datax")
# readRDS("tuits_s.rds") ## Cargar los archivos con tweets
# readRDS("tuits_r.rds") ##
readRDS("tuits_r_df.rds") ##
```


### Extracci√≥n de tweets de un periodo de tiempo especifico
Para extraer tweets de un periodo de tiempo especifico, usaremos la libreria twitteR.
El periodo a extraer ser√° de 2019-10-01 a 2019-11-25
Se extraer√° la m√°xima cantidad de tweets dentro de las limitantes de libreria y la API, apuntando a descargar 30.000 tweets.
```{r Extracci√≥n de tweets en periodo de tiempo usando twitteR}
## Usando package twitteR

# install.package("twitteR")
library(twitteR)

# Setear API app4data de Seba
#?setup_twitter_oauth
setup_twitter_oauth(consumer_key = consumer_key_h,
                    consumer_secret = consumer_secret_h,
                    access_token = access_token_h,
                    access_secret = access_secret_h
                    )

# Buscar tweets con words de la query_4w
#?searchTwitter # de twitteR
#?search_tweets # de rtweet
tuits_4w <- searchTwitter(searchString = query_4w, # Search query to issue to twitter. Use "+" to separate query terms.
              n=1000, # The maximum number of tweets to return
              lang="es", # en espaÒol
              since="2019-10-01", 
              until="2019-11-01", 
              locale=NULL, 
              geocode=NULL, 
              sinceID=NULL, 
              maxID=NULL
             ,resultType="mixed" 
            # ,retryOnRateLimit=5
              )

saveRDS(tuits_4w, file = "tuits_4w.rds") #File, save with encoding, choose BIG5
tuits_4w_df <- twListToDF(tuits_4w)
## Esto no va a resultar porque la API gratuita no entrega m·s all· de los ˙ltimos 9 dÌas.
```

### Limpiar datos para eliminar links innecesarios y palabras no √∫tiles
```{r}
tuits_r_df$clean_text <- gsub("http.*","",  tuits_r_df$text)
tuits_r_df$clean_text <- gsub("https.*","", tuits_r_df$clean_text)
library(dplyr)
library(stopwords)
library(tidytext)

stopwords_es <- tibble(word = c(stopwords(language = "es"),"q", "rt", "d", "si"))
#stopwords_es <- append(stopwords_es$word, values = c("q", "rt", "d", "si")) # palabras frecuentes espec√???ficas a Chile

tuits_limpio <- tuits_r_df %>%
  dplyr::select(clean_text) %>%
  unnest_tokens(word, clean_text)


# Remueve todas las palabras no elementales
tuits_limpio2 <- tuits_limpio %>% 
  anti_join(stopwords_es, by = "word")

# Cuenta todas las palabras √∫nicas ya limpias 
nrow(tuits_limpio2)

```


```{r}
library(ggplot2)
tuits_limpio2 %>%
  count(word, sort = TRUE) %>%
  top_n(20) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x = word, y = n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
  labs(y = "Frecuencia de palabras",
       x = "Palabras",
       title = "Top 20 palabras m·s usadas en tweets",
       subtitle = "Palabras no elementales fueron removidas de la muestra")
```

```{r}
library(lubridate)

head(tuits_r_df$created)
fechas <- lubridate::as_datetime(tuits_r_df$created)
class(fechas)
```


```{r Generar nube de palabras}
library(wordcloud)
library(RColorBrewer)
wordcloud(tuits_limpio2$word, min.freq = 165, max.words=200, random.order = F, rot.per = 0.35, colors=brewer.pal(8, "Dark2"))
```


```{r}
library(openxlsx)

### DESCOMENTAR PARA CARGAR ARCHIVO
# dolar <- read.xlsx("Data_Dolar_IPSA_Cobre.xlsx", sheet = 1, detectDates = TRUE)
# ipsa <- read.xlsx("Data_Dolar_IPSA_Cobre.xlsx", sheet = 2, detectDates = TRUE)
# cobre <- read.xlsx("Data_Dolar_IPSA_Cobre.xlsx", sheet = 3, detectDates = TRUE)
# dolar$fecha <- as_date(dolar$Per√???odo)

# falta hacer bien estos gr√°ficos, con ggplot en lo posible.
plot(x = dolar$fecha, y = dolar$`Valor.(Pesos)`, xlab = "Fecha", ylab = "Valor de USD en CLP", type = "l")
plot(x = ipsa$Fecha, y = ipsa$Cierre, xlab = "Fecha", ylab = "Valor de cierre", type = "l")
plot(x = cobre$Fecha, y = cobre$`Precio.Nominal.Cobre.Diario.(USDc/lb)`, xlab = "Fecha", ylab = "Precio nominal del cobre diario (USD c/lb)", type = "l")

```

------
HASTA AC√Å LLEGU√â... ZZZZ
------

```{r}
## get user IDs of accounts followed by CNN
tmls <- get_timelines(c("TVN", "24horasTVN", "chilevision", "meganoticiascl"), n = 180)

## plot the frequency of tweets for each user over time
tmls %>%
  dplyr::filter(created_at > "2019-10-11") %>%
  dplyr::group_by(screen_name) %>%
  ts_plot("days", trim = 1L) +
  ggplot2::geom_point() +
  ggplot2::theme_minimal() +
  ggplot2::theme(
    legend.title = ggplot2::element_blank(),
    legend.position = "bottom",
    plot.title = ggplot2::element_text(face = "bold")) +
  ggplot2::labs(
    x = NULL, y = NULL,
    title = "Frecuencia de tweets publicados por medios de comunicaci√≥n",
    subtitle = "Twitter status (tweet) counts aggregated by day from October/November 2017",
    caption = "\nSource: Data collected from Twitter's REST API via rtweet"
  )
```

```{r}
sf <- get_trends("Santiago")
?get_trends
```

<!-- ### searchTwitter con fecha desde que parti√≥ el movimiento social  -->
<!-- ```{r} -->
<!-- tweets <- searchTwitter(q = query -->
<!--                         ,n=10, lang="es" -->
<!--                         ,since="2019-10-18" -->
<!--                         ,until="2019-11-18" -->
<!--                         #, geocode='-33.4372, -70.6506, 500km' # geocode = 'latitude,longitude,radius,1mi' # Stgo: -33.4372, -70.6506 -->
<!--                         ,resultType="popular" # s√≥lo los tweets m√°s populares -->
<!--                         ,file_name=filename) -->
<!-- ``` -->


### Revisemos el frame obtenido
```{r}
tmls
streamdata <- tmls
# con 121 segundos se obtuvieron 3973 filas de datos
# 
```

```{r}
table(streamdata$location)
table(streamdata$country)
table(streamdata$coords_coords)
nrow(streamdata)
ts_plot(data = streamdata, by = "secs", tz = "America/Santiago")
```

#clean up links from the data
```{r}
streamdata$clean_text <- gsub("http.*","",  streamdata$text)
streamdata$clean_text <- gsub("https.*","", streamdata$clean_text)
class(stop_words)
#clean out punctuations 
streamdata_clean <- streamdata %>%
  dplyr::select(clean_text) %>%
  unnest_tokens(word, clean_text)

#count unique words
nrow(streamdata_clean)
```

```{r}
library(stopwords)
stopwords_es <- tibble(word = stopwords(language = "es"))
#remove all stop words
streamdata_cleanwords_st <- streamdata_clean %>% anti_join(stopwords_es)
#count all the uniques words again 
nrow(streamdata_cleanwords_st)
```


```{r}
streamdata_cleanwords_st %>%
  count(word, sort = TRUE) %>%
  top_n(10) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x = word, y = n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
  labs(y = "Frequency of Words",
       x = "words",
       title = "Top 10 most used words in tweets",
       subtitle = "Stop words have been removed")
```


### Se crea una wordcloud
NOTA: se debe cambiar la min.freq para que no aparezcan palabras con muy pocas repeticiones para el espacio del plot
Tambi√É¬©n se debe seguir limpiando los datos de palabras para que no aparezcan palabras asiaticas o en idiomas que no nos interesen
```{r}
install.packages("wordcloud")
install.packages("tm")
install.packages("ColorBrewer")
library(wordcloud)
library(tm)
library(ColorBrewer)
```

### Ploteamos el wordcloud
```{r, echo=FALSE}
wordcloud(streamdata_cleanwords_st$word
          ,min.freq = 42
          ,random.order = FALSE
          ,colors=brewer.pal(6,"Dark2"))
```


### An√É¬°lisis de sentimiento
Basandose en [este link](https://rpubs.com/JaimeFC/415349)
Tambi√É¬©n revisar [tiditext](https://www.tidytextmining.com/sentiment.html)
Y tambi√É¬©n [SentimentAnalysis](https://cran.r-project.org/web/packages/SentimentAnalysis/vignettes/SentimentAnalysis.html#language-support-and-extensibility)


<!-- ## An√°lisis de Sentimiento -->
<!-- ```{r} -->
<!-- install.packages("sentimentr") -->
<!-- library(sentimentr) -->

<!-- install.packages("remotes") -->

<!-- remotes::install_github("abhy/sentiment") -->
<!-- library(SentimentAnalysis) -->

<!-- analyzeSentiment(streamdata_cleanwords_st, language = "spanish") -->
<!-- ``` -->

